{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Theoretical Questions"
      ],
      "metadata": {
        "id": "BDXVXKqU32ZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems?"
      ],
      "metadata": {
        "id": "sfF2j3vl32wG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Bagging Regressors work similarly to Bagging Classifiers, but instead of taking a majority vote, they average the predictions of the individual base regressors."
      ],
      "metadata": {
        "id": "RsUi0DgCOMyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between multiple model training and single model training."
      ],
      "metadata": {
        "id": "XQQAzl1w4Qdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Multiple model training, often referred to as ensemble learning, involves training several individual models and combining their predictions to make a final prediction. This can help reduce variance and improve robustness compared to single model training"
      ],
      "metadata": {
        "id": "aTx0mekvOgnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of feature randomness in Random Forest."
      ],
      "metadata": {
        "id": "8fd31ppm4SZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> In Random Forest, feature randomness (also called feature bagging or subspace sampling) means that at each split in a decision tree, only a random subset of the features is considered for finding the best split. This adds another layer of randomness on top of the sample bagging."
      ],
      "metadata": {
        "id": "SBVaYe2aOo92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is OOB (Out-of-Bag) Score"
      ],
      "metadata": {
        "id": "sIMabU2X4VUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The Out-of-Bag (OOB) score is a way to estimate the performance of a Bagging model (like Random Forest) without using a separate validation set. Since each base estimator in Bagging is trained on a bootstrap sample of the data, there will be some data points that are not included in that sample. These \"out-of-bag\" samples can be used to evaluate the performance of the base estimator."
      ],
      "metadata": {
        "id": "zrkk-KJtOwPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How can you measure the importance of features in a Random Forest model"
      ],
      "metadata": {
        "id": "cfG_mnh74X4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> This is the default method in scikit-learn. It measures how much each feature decreases the weighted impurity (like Gini impurity for classification or MSE for regression) across all trees in the forest. Features that contribute to larger decreases in impurity are considered more important."
      ],
      "metadata": {
        "id": "xLgvNlhVO3aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the working principle of a Bagging Classifier"
      ],
      "metadata": {
        "id": "q3zYGxlJ4Zkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=>  This method, also known as permutation importance, is generally considered more reliable. It involves shuffling the values of a feature for the out-of-bag samples and measuring the decrease in the model's accuracy. A large decrease in accuracy indicates that the feature is important for the model's predictions."
      ],
      "metadata": {
        "id": "rC-wZ4IHO-xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you evaluate a Bagging Classifierâ€™s performance"
      ],
      "metadata": {
        "id": "FwJeAMdk4bJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Accuracy: The proportion of correctly classified instances.\n",
        "\n",
        "Precision: The ability of the classifier not to label as positive a sample that is negative.\n",
        "\n",
        "Recall (Sensitivity): The ability of the classifier to find all the positive samples.\n",
        "\n",
        "F1-Score: The harmonic mean of Precision and Recall, providing a balanced measure.\n",
        "\n",
        "ROC-AUC Score: The Area Under the Receiver Operating Characteristic Curve, which measures the classifier's ability to distinguish between classes.\n",
        "\n",
        "Confusion Matrix: A table that summarizes the number of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "Out-of-Bag (OOB) Score: As mentioned earlier, this provides an internal estimate of the model's performance without needing a separate validation set."
      ],
      "metadata": {
        "id": "Kzfm5SOrPIQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How does a Bagging Regressor work"
      ],
      "metadata": {
        "id": "eo2QoCbN4dd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Bagging Regressors work similarly to Bagging Classifiers, but instead of taking a majority vote, they average the predictions of the individual base regressors."
      ],
      "metadata": {
        "id": "Y-diaAkxPRjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main advantage of ensemble techniques"
      ],
      "metadata": {
        "id": "wE8vbXGl4f1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The main advantage of ensemble techniques is that they can significantly improve the overall performance and robustness of a model compared to using a single model. By combining the predictions of multiple models, ensembles can reduce variance, bias, or both, leading to better generalization to unseen data and increased accuracy. They are particularly effective at reducing overfitting and are less sensitive to noisy data."
      ],
      "metadata": {
        "id": "zF4ZF39ZPXRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the main challenge of ensemble methods"
      ],
      "metadata": {
        "id": "0pruXqHG4hsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The main challenge of ensemble methods is often their interpretability and computational cost.\n",
        "\n",
        "Interpretability: Understanding why an ensemble model makes a particular prediction can be difficult because it's a combination of multiple individual models. This is especially true for complex ensembles like Random Forests or Gradient Boosting, where the decision-making process is not as transparent as a single decision tree.\n",
        "\n",
        "Computational Cost: Training and storing multiple models, as well as making predictions with them, can be computationally more expensive and require more memory compared to training and using a single model. This can be a limitation for very large datasets or real-time applications where prediction speed is critical."
      ],
      "metadata": {
        "id": "mQK3itusPeL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Explain the key idea behind ensemble techniques"
      ],
      "metadata": {
        "id": "obut2SwN4jy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=>The key idea behind ensemble techniques is to combine the predictions of multiple individual models to produce a more accurate and robust prediction than any single model could achieve on its own."
      ],
      "metadata": {
        "id": "AYhnfN9ZPn55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is a Random Forest Classifier?"
      ],
      "metadata": {
        "id": "m2tdP0aq4lZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Random Forest, feature randomness (also called feature bagging or subspace sampling) means that at each split in a decision tree, only a random subset of the features is considered for finding the best split. This adds another layer of randomness on top of the sample bagging, further decorrelating the trees and making the ensemble more robust to noisy features and preventing overfitting."
      ],
      "metadata": {
        "id": "__jmO8KvPtw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are the main types of ensemble techniques"
      ],
      "metadata": {
        "id": "Uj07CZzv4nz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> This method involves training multiple models of the same type on different bootstrap samples (randomly sampled subsets with replacement) of the training data. The final prediction is typically the average of the individual model predictions (for regression) or the majority vote (for classification). Random Forest is a popular example of a bagging technique that uses decision trees as base estimators and also incorporates feature randomness."
      ],
      "metadata": {
        "id": "OFn8TKSlTPhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is ensemble learning in machine learning"
      ],
      "metadata": {
        "id": "G_9BQpx54peo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> This method trains models sequentially, where each subsequent model is trained to correct the errors made by the previous models. It focuses on the data points that were misclassified or poorly predicted by the earlier models. Examples include AdaBoost, Gradient Boosting (like Gradient Boosting Machines - GBM), and XGBoost."
      ],
      "metadata": {
        "id": "gJieHjt-TTEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should we avoid using ensemble methods"
      ],
      "metadata": {
        "id": "306sx6Q04rNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> This method involves training a meta-model (or blender) that learns to combine the predictions of several base models. The base models are trained on the original training data, and their predictions are then used as input features for the meta-model. The meta-model is trained to make the final prediction based on the predictions of the base models."
      ],
      "metadata": {
        "id": "WnnY8S3ITXJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How does Bagging help in reducing overfitting."
      ],
      "metadata": {
        "id": "jh9YTP-m4s3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> This method involves training multiple models of the same type on different bootstrap samples (randomly sampled subsets with replacement) of the training data. The final prediction is typically the average of the individual model predictions (for regression) or the majority vote (for classification). Random Forest is a popular example of a bagging technique that uses decision trees as base estimators and also incorporates feature randomness."
      ],
      "metadata": {
        "id": "79C3hU5ETcS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is Random Forest better than a single Decision Tree"
      ],
      "metadata": {
        "id": "hrO5Tc3u4unz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Random Forest is generally better than a single Decision Tree for several key reasons, primarily related to reducing overfitting and improving robustness:\n",
        "\n",
        "1. Reduced Variance: Single decision trees can be prone to overfitting, especially when they are deep. They can learn the training data too well, including the noise, and perform poorly on unseen data. Random Forest mitigates this by training multiple trees on different bootstrap samples of the data and averaging or taking a majority vote of their predictions. This averaging process reduces the variance of the model.\n",
        "2. Reduced Overfitting: The combination of bootstrap sampling (sampling data with replacement) and feature randomness (considering only a random subset of features at each split) in Random Forest helps to decorrelate the individual trees. This prevents the trees from all making the same errors and reduces their tendency to overfit the training data."
      ],
      "metadata": {
        "id": "_Drhaa5ZTo8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the role of bootstrap sampling in Bagging"
      ],
      "metadata": {
        "id": "cvCaBcpn4wko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that improves the accuracy and stability of machine learning models. It works by training multiple models on different subsets of the training data and then aggregating their predictions to produce a final output. This method is particularly effective for high-variance models, such as decision trees, which are prone to overfitting."
      ],
      "metadata": {
        "id": "UC05YSB-Tvlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What are some real-world applications of ensemble techniques"
      ],
      "metadata": {
        "id": "V7w1qPjU4yV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Ensemble techniques are widely used in various real-world applications due to their ability to improve model performance and robustness. Here are some examples:\n",
        "\n",
        "* Healthcare: Ensemble methods are used for disease diagnosis, medical image analysis, and predicting patient outcomes. For instance, they can combine predictions from different models to improve the accuracy of identifying cancerous tumors or predicting the risk of heart disease.\n",
        "* Finance: Ensemble techniques are employed in credit scoring, fraud detection, stock market prediction, and algorithmic trading. By combining multiple models, financial institutions can make more accurate predictions and better manage risk."
      ],
      "metadata": {
        "id": "eA6aWuJHT63t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "1wJ_wMWl4zsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Bagging (Bootstrap Aggregating):\n",
        "\n",
        "* Parallel Training: Models are trained independently of each other on different bootstrap samples of the training data.\n",
        "* Reduces Variance: Bagging primarily aims to reduce the variance of the model by averaging or voting the predictions of multiple models.\n",
        "* Diverse Models: Since each model is trained on a different subset of the data, the individual models tend to be diverse.\n",
        "* Simple Aggregation: Predictions are combined through simple averaging (regression) or majority voting (classification).\n",
        "\n",
        "Boosting:\n",
        "\n",
        "* Sequential Training: Models are trained sequentially, with each new model focusing on correcting the errors made by the previous models.\n",
        "* Reduces Bias: Boosting primarily aims to reduce the bias of the model by giving more weight to data points that were misclassified or poorly predicted by earlier models.\n",
        "* Dependent Models: Each subsequent model is dependent on the previous ones, as it tries to improve upon their performance.\n",
        "* Weighted Aggregation: Predictions are combined with weights, where models that perform better are given more weight."
      ],
      "metadata": {
        "id": "gZyFg0SSUD6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practical Questions"
      ],
      "metadata": {
        "id": "h4d2ixjy407Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"
      ],
      "metadata": {
        "id": "CRXBsLSp5BMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "!pip install -U scikit-learn==1.2.2\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_classifier = BaggingClassifier(base_estimator=decision_tree, n_estimators=10, random_state=42)\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "y\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "WmvBvZmBUfdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "_KpE3dUx5N-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "decision_tree = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "bagging_regressor = BaggingRegressor(base_estimator=decision_tree, n_estimators=10, random_state=42)\n",
        "\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "SPpjviAEUkg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
      ],
      "metadata": {
        "id": "BVH4TY2_5Plm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = load_breast_cancer()\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "random_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "feature_importances = random_forest_classifier.feature_importances_\n",
        "\n",
        "for feature_name, importance in zip(data.feature_names, feature_importances):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n",
        ""
      ],
      "metadata": {
        "id": "JdOztSx0UlIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a Random Forest Regressor and compare its performance with a single Decision Tree"
      ],
      "metadata": {
        "id": "4d7DJQWQ5RKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "random_forest_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "random_forest_regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = random_forest_regressor.predict(X_test)\n",
        "\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "decision_tree_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "decision_tree_regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred_dt = decision_tree_regressor.predict(X_test)\n",
        "\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "print(f\"Random Forest MSE: {mse_rf:.2f}\")\n",
        "print(f\"Decision Tree MSE: {mse_dt:.2f}\")\n"
      ],
      "metadata": {
        "id": "36eD47dKUloR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
      ],
      "metadata": {
        "id": "ZuYFvp6N5TE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  from sklearn.datasets import make_classification\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.metrics import accuracy_score\n",
        "\n",
        "  X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42, oob_score=True)\n",
        "  random_forest_classifier.fit(X_train, y_train)\n",
        "  oob_score = random_forest_classifier.oob_score_\n",
        "  print(f\"Out-of-Bag (OOB) Score: {oob_score:.4f}\")\n",
        "  y_pred = random_forest_classifier.predict(X_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        ""
      ],
      "metadata": {
        "id": "jm0uwp1ZUmAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n"
      ],
      "metadata": {
        "id": "R9YSSi9_5UfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_classifier = SVC\n",
        "\n",
        "bagging_classifier = BaggingClassifier(base_estimator=svm_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "44VK-eOFUmXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Train a Random Forest Classifier with different numbers of trees and compare accuracy"
      ],
      "metadata": {
        "id": "v_61v46_5Xpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "num_trees = [10, 50, 100, 200]\n",
        "\n",
        "for n_trees in num_trees:\n",
        "    random_forest_classifier = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    random_forest_classifier.fit(X_train, y_train)\n",
        "    y_pred = random_forest_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Number of Trees: {n_trees}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "5VTD34hoUmvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score"
      ],
      "metadata": {
        "id": "XOKg__6R5ZLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "num_trees = [10, 50, 100, 200]\n",
        "\n",
        "for n_trees in num_trees:\n",
        "    random_forest_classifier = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    random_forest_classifier.fit(X_train, y_train)\n",
        "    y_pred = random_forest_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Number of Trees: {n_trees}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "FdEUdKW5UnHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a Random Forest Regressor and analyze feature importance scores"
      ],
      "metadata": {
        "id": "3kN3aiCv5asc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "num_trees = [10, 50, 100, 200]\n",
        "\n",
        "for n_trees in num_trees:\n",
        "    random_forest_classifier = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    random_forest_classifier.fit(X_train, y_train)\n",
        "    y_pred = random_forest_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Number of Trees: {n_trees}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "ZIHYnm4TUntt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "9KGh3JvF5ciG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "num_trees = [10, 50, 100, 200]\n",
        "\n",
        "for n_trees in num_trees:\n",
        "    random_forest_classifier = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    random_forest_classifier.fit(X_train, y_train)\n",
        "    y_pred = random_forest_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Number of Trees: {n_trees}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Zz0V3a-sUoWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV"
      ],
      "metadata": {
        "id": "V5aSJ0O15jLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "num_trees = [10, 50, 100, 200]\n",
        "\n",
        "for n_trees in num_trees:\n",
        "    random_forest_classifier = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    random_forest_classifier.fit(X_train, y_train)\n",
        "    y_pred = random_forest_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Number of Trees: {n_trees}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Z5t32r64UovQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a Bagging Regressor with different numbers of base estimators and compare performance"
      ],
      "metadata": {
        "id": "xetbxScT52bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "num_trees = [10, 50, 100, 200]\n",
        "\n",
        "for n_trees in num_trees:\n",
        "    random_forest_classifier = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    random_forest_classifier.fit(X_train, y_train)\n",
        "    y_pred = random_forest_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Number of Trees: {n_trees}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "XCgMNCo3UpFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Train a Random Forest Classifier and analyze misclassified samples"
      ],
      "metadata": {
        "id": "oe2kC_9e54WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "num_trees = [10, 50, 100, 200]\n",
        "\n",
        "for n_trees in num_trees:\n",
        "    random_forest_classifier = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    random_forest_classifier.fit(X_train, y_train)\n",
        "    y_pred = random_forest_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Number of Trees: {n_trees}, Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "wmom12hrUpdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier"
      ],
      "metadata": {
        "id": "KqC64P3H56QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_classifier = BaggingClassifier(base_estimator=decision_tree, n_estimators=10, random_state=42)\n",
        "\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tree = decision_tree.predict(X_test)\n",
        "y_pred_bagging = bagging_classifier.predict(X_test)\n",
        "\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {accuracy_tree:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")\n"
      ],
      "metadata": {
        "id": "pK-CxTdnUp1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a Random Forest Classifier and visualize the confusion matrix"
      ],
      "metadata": {
        "id": "TcdRSbLN58Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_classifier = BaggingClassifier(base_estimator=decision_tree, n_estimators=10, random_state=42)\n",
        "\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tree = decision_tree.predict(X_test)\n",
        "y_pred_bagging = bagging_classifier.predict(X_test)\n",
        "\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {accuracy_tree:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")"
      ],
      "metadata": {
        "id": "tqtJP5FYUqXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "ZxLrBDbf59xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_classifier = BaggingClassifier(base_estimator=decision_tree, n_estimators=10, random_state=42)\n",
        "\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tree = decision_tree.predict(X_test)\n",
        "y_pred_bagging = bagging_classifier.predict(X_test)\n",
        "\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {accuracy_tree:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")"
      ],
      "metadata": {
        "id": "NB27V3gbUq6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a Random Forest Classifier and print the top 5 most important features"
      ],
      "metadata": {
        "id": "FJNt2g-45_dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_classifier = BaggingClassifier(base_estimator=decision_tree, n_estimators=10, random_state=42)\n",
        "\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tree = decision_tree.predict(X_test)\n",
        "y_pred_bagging = bagging_classifier.predict(X_test)\n",
        "\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {accuracy_tree:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")"
      ],
      "metadata": {
        "id": "B095IGpLUrP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
      ],
      "metadata": {
        "id": "iswM48wd6BM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_classifier = BaggingClassifier(base_estimator=decision_tree, n_estimators=10, random_state=42)\n",
        "\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tree = decision_tree.predict(X_test)\n",
        "y_pred_bagging = bagging_classifier.predict(X_test)\n",
        "\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {accuracy_tree:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")"
      ],
      "metadata": {
        "id": "Zb25Yky3Urqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
      ],
      "metadata": {
        "id": "62KC7H9I6Dkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_classifier = BaggingClassifier(base_estimator=decision_tree, n_estimators=10, random_state=42)\n",
        "\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tree = decision_tree.predict(X_test)\n",
        "y_pred_bagging = bagging_classifier.predict(X_test)\n",
        "\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {accuracy_tree:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")"
      ],
      "metadata": {
        "id": "EPx6QFVdUsC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance"
      ],
      "metadata": {
        "id": "wdNYxuUC6Fin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "decision_tree_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "\n",
        "bagging_tree = BaggingRegressor(base_estimator=decision_tree_regressor, n_estimators=10, random_state=42)\n",
        "bagging_knn = BaggingRegressor(base_estimator=knn_regressor, n_estimators=10, random_state=42)\n",
        "\n",
        "bagging_tree.fit(X_train, y_train)\n",
        "\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tree = bagging_tree.predict(X_test)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "\n",
        "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "print(f\"Bagging Regressor with Decision Tree MSE: {mse_tree:.2f}\")\n",
        "print(f\"Bagging Regressor with KNeighbors MSE: {mse_knn:.2f}\")\n"
      ],
      "metadata": {
        "id": "9k9A9Ui0Usn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score"
      ],
      "metadata": {
        "id": "gSrik6aG6HTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "y_prob = random_forest.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f\"Random Forest Classifier ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "id": "IE-l-MqLUtJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a Bagging Classifier and evaluate its performance using cross-validatio"
      ],
      "metadata": {
        "id": "uMlqYSkT6JCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "bagging_classifier = BaggingClassifier(base_estimator=decision_tree, n_estimators=10, random_state=42)\n",
        "\n",
        "decision_tree.fit(X_train, y_train)\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tree = decision_tree.predict(X_test)\n",
        "y_pred_bagging = bagging_classifier.predict(X_test)\n",
        "\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {accuracy_tree:.2f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")"
      ],
      "metadata": {
        "id": "JKuOGYGUXzKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a Random Forest Classifier and plot the Precision-Recall curve"
      ],
      "metadata": {
        "id": "DMN1RY6Q6Kir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "y_prob = random_forest.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "91qBV3d4Xwuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy"
      ],
      "metadata": {
        "id": "A38c4DeS6O8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(random_state=42))\n",
        "]\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "stacking_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred_stacking = stacking_classifier.predict(X_test)\n",
        "\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_stacking:.2f}\")"
      ],
      "metadata": {
        "id": "xBQ_74fdXsij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "GdgZfAj26QbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the above Ques.\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "x,y=make_regression(n_samples=1000,n_features=10,random_state=42)\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
        "\n",
        "decision_tree_regressor=DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "bagging_regressor=BaggingRegressor(base_estimator=decision_tree_regressor,n_estimators=10,random_state=42)\n",
        "\n",
        "bagging_regressor.fit(x_train,y_train)\n",
        "\n",
        "y_pred=bagging_regressor.predict(x_test)\n",
        "\n",
        "mse=mean_squared_error(y_test,y_pred)\n",
        "\n",
        "print(f\"Bagging Regressor MSE: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "nKxIV4bnXpjA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}